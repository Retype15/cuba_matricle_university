import base64
import os
import io
import pandas as pd
from google import genai
from google.genai import types
import plotly.graph_objects as go
import streamlit as st
import json
import time
import numpy as np

# --- Configuraci칩n de la API de Gemini ---
@st.cache_resource(show_spinner=False, ttl=3600)
def configure_gemini_client():
    api_key = os.environ.get("GEMINI_API_KEY")
    if not api_key:
        if "GEMINI_API_KEY" in st.secrets:
            api_key = st.secrets["GEMINI_API_KEY"]
        else:
            st.error("Error: GEMINI_API_KEY no encontrada. Config칰rala en las variables de entorno o en .streamlit/secrets.toml.")
            return None
    return genai.Client(api_key=api_key)

gemini_client = configure_gemini_client()

# --- Funciones Auxiliares para Conversi칩n de Contexto ---
def _clean_fig_dict_for_json(d):
    """Convierte ndarrays a listas para serializaci칩n JSON."""
    if isinstance(d, dict):
        return {k: _clean_fig_dict_for_json(v) for k, v in d.items()}
    elif isinstance(d, list):
        return [_clean_fig_dict_for_json(v) for v in d]
    elif isinstance(d, np.ndarray):
        return d.tolist()
    else:
        return d

def _convert_context_to_gemini_parts(context_list):
    parts = []
    for item_idx, item in enumerate(context_list):
        if item is None: continue
        if isinstance(item, str):
            parts.append(types.Part.from_text(text=item))
        elif isinstance(item, pd.DataFrame):
            try:
                json_data = item.to_json(orient="records", indent=2)
                parts.append(types.Part.from_text(text=f"Datos de Tabla (Contexto {item_idx+1}, formato JSON):\n```json\n{json_data}\n```"))
            except Exception as e:
                st.warning(f"Error al convertir DataFrame a JSON para IA: {e}")
                parts.append(types.Part.from_text(text="[ERROR AL PROCESAR DATAFRAME]"))
        elif isinstance(item, go.Figure):
            try:
                fig_dict = item.to_dict()
                clean_dict = _clean_fig_dict_for_json(fig_dict)
                fig_json = json.dumps(clean_dict, indent=2)
                parts.append(types.Part.from_text(text=f"Descripci칩n de Gr치fico Plotly (Contexto {item_idx+1}, formato JSON):\n```json\n{fig_json}\n```"))
            except Exception as e:
                st.warning(f"Error al convertir gr치fico Plotly a JSON para IA: {e}")
                parts.append(types.Part.from_text(text=f"[GR츼FICO NO CONVERTIDO A JSON: {str(e)}]"))
        else:
            st.warning(f"Tipo de contexto no soportado para IA: {type(item)}. Se ignorar치.")
    return parts

# --- Funci칩n Principal para Interactuar con Gemini ---
def generate_ai_response_stream(user_message, conversation_history_for_gemini, system_instruction_text, context_list_for_first_turn=None):
    if not gemini_client:
        yield ("error", "El asistente de IA no est치 configurado correctamente.")
        return

    current_user_content_parts = [types.Part.from_text(text=user_message)]
    if context_list_for_first_turn:
        context_parts_ready = _convert_context_to_gemini_parts(context_list_for_first_turn)
        current_user_content_parts = context_parts_ready + current_user_content_parts
        
    final_contents_for_gemini = conversation_history_for_gemini + [types.Content(role="user", parts=current_user_content_parts)]

    generate_content_config = types.GenerateContentConfig(
        response_mime_type="text/plain",
        tools=[types.Tool(code_execution=types.ToolCodeExecution)],
        system_instruction=[types.Part.from_text(text=system_instruction_text)],
    )

    try:
        stream = gemini_client.models.generate_content_stream(
            model="gemini-2.5-flash-preview-05-20",
            contents=final_contents_for_gemini,
            config=generate_content_config,
        )
        for chunk in stream:
            if not chunk.candidates: continue
            for part in chunk.candidates[0].content.parts:
                if part.text:
                    yield ("text", part.text)
                elif part.executable_code:
                    yield ("code", part.executable_code.code)
                elif part.code_execution_result:
                    if hasattr(part.code_execution_result, 'outcome') and part.code_execution_result.outcome == "OUTCOME_OK":
                        if hasattr(part.code_execution_result, 'output') and isinstance(part.code_execution_result.output, types.Blob):
                             yield ("image", part.code_execution_result.output.data, part.code_execution_result.output.mime_type)
                        else:
                             yield ("result", str(part.code_execution_result.output))
                    else:
                         yield ("result", f"Error en ejecuci칩n: {str(part.code_execution_result.outcome)}")
                elif hasattr(part, 'inline_data') and part.inline_data.data:
                    yield ("image", part.inline_data.data, part.inline_data.mime_type)
    except Exception as e:
        st.error(f"Error en la comunicaci칩n con Gemini: {e}")
        yield ("error", f"Error al comunicarse con el asistente de IA: {e}.")

# --- El Componente de Chat para Streamlit (con Bot칩n de Reinicio Integrado) ---
def ask_ai_component(analysis_context, key, extra_data=None):
    
    with st.expander(f"游뱄 쯇reguntas sobre este an치lisis? 춰Preg칰ntale al Asistente de IA!", expanded=False):
        
        history_key = f"messages_{key}"
        gemini_history_key = f"gemini_messages_{key}"

        # Inicializaci칩n de historiales si no existen
        if history_key not in st.session_state:
            st.session_state[history_key] = [{"role": "assistant", "content": "춰Hola! Estoy listo para responder tus preguntas sobre el an치lisis que est치s viendo."}]
        if gemini_history_key not in st.session_state:
            st.session_state[gemini_history_key] = []

        # Mostrar historial del chat
        for message in st.session_state[history_key]:
            with st.chat_message(message["role"]):
                # Solo mostrar texto o im치genes en el historial. El c칩digo/resultado se ignora.
                if isinstance(message["content"], dict):
                    if message["content"]["type"] == "image":
                        mime_type = message["content"].get("mime_type", "image/png")
                        st.image(message["content"]["data"], caption=f"Imagen generada ({mime_type})")
                else: # Es un string de texto simple
                    st.markdown(message["content"])

        # System Prompt (se mantiene igual, aunque no mostremos el c칩digo, la IA a칰n lo usa)
        system_instruction_for_ai = """
        Eres un asistente de an치lisis de datos altamente eficiente, experto en el sistema de educaci칩n superior de Cuba. Tu objetivo es responder a las preguntas del usuario de forma clara y precisa, bas치ndote EXCLUSIVAMENTE en el contexto que se te proporciona.

        **Directrices de An치lisis:**
        1.  **Contexto:** Recibir치s contexto en forma de texto y datos estructurados en formato JSON (para tablas y gr치ficos).
        2.  **Procesamiento de Datos:** Tu primer paso debe ser interpretar los datos JSON y cargarlos en un DataFrame de pandas para facilitar cualquier c치lculo o an치lisis. S칠 directo y eficiente en tu c칩digo, por lo que no escribas el json, sino los datos m치s importantes de forma compacta, usando listas o diccionarios.
        3.  **Ejecuci칩n de C칩digo:** Tienes acceso a una herramienta para ejecutar c칩digo de Python. 칔sala para realizar c치lculos, analizar datos o generar nuevas visualizaciones.

        **Generaci칩n de Gr치ficos:**
        - Para crear cualquier visualizaci칩n, usa **exclusivamente la biblioteca `matplotlib.pyplot`**.
        - Para mostrar el gr치fico, simplemente **usa `plt.show()` al final de tu script de graficaci칩n**. El sistema capturar치 autom치ticamente la imagen y la mostrar치 al usuario.

        **Estructura de la Respuesta:**
        - Ejecuta el c칩digo necesario para responder, pero en tu respuesta final al usuario da un resumen claro y conciso de los resultados.
        - Si no necesitas c칩digo, responde directamente con texto.
        - No inventes informaci칩n. Si la respuesta no est치 en el contexto, ind칤calo amablemente.
        """
        
        current_textual_context = f"Contexto textual del an치lisis actual:\n---\n{analysis_context}\n---"
        
        # --- Contenedor para el Input y el Bot칩n de Reinicio ---
        input_container = st.container()
        with input_container:
            col_reset, col_input = st.columns([1, 20]) # Proporci칩n para un bot칩n peque침o y un input grande

            with col_reset:
                # El bot칩n necesita un poco de espacio superior para alinearse visualmente con el chat_input
                #st.markdown("<div style='margin-top: 0px;'></div>", unsafe_allow_html=True) 
                if st.button("游댃", key=f"reset_chat_{key}", help="Reiniciar esta conversaci칩n"):
                    st.session_state[history_key] = []
                    st.session_state[gemini_history_key] = []
                    st.rerun()

            with col_input:
                prompt = st.chat_input("Escribe tu pregunta aqu칤...", key=f"chat_input_{key}")

        if prompt:
            st.session_state[history_key].append({"role": "user", "content": prompt})
            
            ai_context_to_send = [current_textual_context]
            if not st.session_state[gemini_history_key] and extra_data:
                ai_context_to_send.extend(extra_data)
            
            with st.chat_message("assistant"):
                with st.spinner("游 El asistente est치 trabajando en tu respuesta..."):
                    response_container = st.container()
                    full_text_response = ""

                    for response_type, content, *extra_info in generate_ai_response_stream(
                        user_message=prompt, 
                        conversation_history_for_gemini=st.session_state[gemini_history_key],
                        system_instruction_text=system_instruction_for_ai, 
                        context_list_for_first_turn=ai_context_to_send
                    ):
                        # SOLO RENDERIZAR IM츼GENES Y ACUMULAR TEXTO
                        if response_type == "image":
                            mime_type = extra_info[0] if extra_info else "image/png"
                            response_container.image(content, caption=f"Imagen generada ({mime_type})")
                            # Guardar la imagen en el historial de visualizaci칩n
                            st.session_state[history_key].append({"role": "assistant", "content": {"type": "image", "data": content, "mime_type": mime_type}})
                        
                        elif response_type == "text":
                            full_text_response += content
                        
                        elif response_type == "error":
                            st.error(content)
                            full_text_response = content
                        
                        # Ignorar "code" y "result" para la visualizaci칩n

                # Mostrar el texto final acumulado
                if full_text_response:
                    response_container.markdown(full_text_response)
            
            # Guardar la respuesta final en el historial de visualizaci칩n
            if full_text_response:
                st.session_state[history_key].append({"role": "assistant", "content": full_text_response})
            
            # Actualizar el historial t칠cnico para Gemini
            user_turn_parts = _convert_context_to_gemini_parts([prompt] + (ai_context_to_send if not st.session_state[gemini_history_key] else []))
            st.session_state[gemini_history_key].append(types.Content(role="user", parts=user_turn_parts))
            st.session_state[gemini_history_key].append(types.Content(role="model", parts=[types.Part.from_text(text=full_text_response)]))
            
            st.rerun()